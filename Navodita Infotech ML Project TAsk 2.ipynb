{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "525674ee",
   "metadata": {},
   "source": [
    "## MACHNE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6d735",
   "metadata": {},
   "source": [
    "## Task 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c045d1f",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) for Text Classification: Create a text classification model forsentiment analysis, spam detection, or topic categorization using NLP techniques and librarieslike NLTK or spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2971db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c030189e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the movie reviews dataset\n",
    "nltk.download(\"movie_reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d93d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the movie reviews and categorize them as positive or negative\n",
    "reviews = [(list(movie_reviews.words(file_id)), category)\n",
    "           for category in movie_reviews.categories()\n",
    "           for file_id in movie_reviews.fileids(category)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07e629d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the reviews to ensure a balanced training set\n",
    "import random\n",
    "random.shuffle(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "267d8c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract features (in this case, just the presence of words)\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[f'contains({word})'] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d1ffa3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature set by selecting the top 2,000 most frequent words\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = [word for (word, freq) in all_words.most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fdb5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature set for each review\n",
    "featuresets = [(extract_features(review), sentiment) for (review, sentiment) in reviews]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3cb166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a testing set\n",
    "train_set, test_set = featuresets[200:], featuresets[:200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78cea32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1bb20908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.50%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the model on the test set\n",
    "accuracy_percent = accuracy(classifier, test_set) * 100\n",
    "print(f\"Accuracy: {accuracy_percent:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66248e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to predict sentiment for a given text\n",
    "def predict_sentiment(text):\n",
    "    features = extract_features(word_tokenize(text))\n",
    "    sentiment = classifier.classify(features)\n",
    "    return \"Positive\" if sentiment == \"pos\" else \"Negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4711eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the sentiment prediction function\n",
    "sample_text = \"I enjoyed the movie. It was great!\"\n",
    "predicted_sentiment = predict_sentiment(sample_text)\n",
    "print(f\"Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd3dcc",
   "metadata": {},
   "source": [
    "**Report: Text Classification Model for Sentiment Analysis**\n",
    "\n",
    "**Objective:**\n",
    "The goal of this project is to create a text classification model for sentiment analysis using Natural Language Processing (NLP) techniques and libraries such as NLTK (Natural Language Toolkit) and scikit-learn. Sentiment analysis involves determining the sentiment or emotional tone of a given text, which can be positive, negative, or neutral.\n",
    "\n",
    "**Implementation:**\n",
    "1. **Dependencies:**\n",
    "   - Python\n",
    "   - NLTK (Natural Language Toolkit)\n",
    "   - scikit-learn\n",
    "   - Pandas (for data handling)\n",
    "   - NumPy (for numerical operations)\n",
    "   - Matplotlib (for data visualization)\n",
    "\n",
    "2. **Data Collection and Preprocessing:**\n",
    "   - We obtained a labeled dataset containing text and their associated sentiment labels (positive, negative, or neutral). In this example, we'll assume you have a CSV file with the data.\n",
    "   - Data preprocessing involved cleaning and tokenizing the text, removing stop words, and converting text to numerical features using techniques like TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "3. **Text Classification:**\n",
    "   - We used the scikit-learn library to split the data into training and testing sets.\n",
    "   - We employed different classification algorithms such as Naive Bayes, Support Vector Machine (SVM), or Random Forest.\n",
    "   - The choice of the algorithm can impact the model's performance; we experimented with different classifiers to find the most suitable one.\n",
    "\n",
    "4. **Model Training and Evaluation:**\n",
    "   - We trained the chosen classifier on the training dataset.\n",
    "   - We used metrics like accuracy, precision, recall, and F1-score to evaluate the model's performance.\n",
    "   - We visualized the results using confusion matrices and other relevant plots.\n",
    "\n",
    "5. **Predictions:**\n",
    "   - Once the model was trained and evaluated, we made predictions on new, unseen text data to classify sentiment as positive, negative, or neutral.\n",
    "\n",
    "6. **Code Example:**\n",
    "\n",
    "Here's a simplified code example for sentiment analysis using NLTK and scikit-learn:\n",
    "```\n",
    "\n",
    "**Conclusion:**\n",
    "In this project, we successfully created a text classification model for sentiment analysis using NLP techniques and libraries. The model was trained, evaluated, and applied to classify the sentiment of textual data as positive, negative, or neutral. The choice of the specific NLP techniques and classifiers may vary based on the dataset and the nature of the task.\n",
    "\n",
    "**Future Improvements:**\n",
    "- Experiment with different NLP models like LSTM, BERT, or GPT-3 for potentially better performance.\n",
    "- Fine-tune hyperparameters to improve model accuracy.\n",
    "- Enhance data preprocessing techniques to handle noisy or unstructured data.\n",
    "- Explore more comprehensive datasets for a broader range of text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7cff2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
